# -*- coding: utf-8 -*-
"""TextAnalytics_2201277.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Gey9shFYLxwZhUFhgTxAw3196jIk0CE

### Offensive Language Identification Dataset - OLID
"""

#Mounting google drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

import numpy as np
import os
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
vectorizer = TfidfVectorizer()
le = LabelEncoder()

"""**Let's put your student id as a variable, that you will use different places**"""

student_id = 2201277

"""Let's set `seed` for all libraries like `torch`, `numpy` etc as my student id"""

#numpy seed
np.random.seed(student_id)

import os

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('/content/gdrive/MyDrive/CE807_7_SP/assignment2',str(student_id)) # Make sure to update with your student_id and student_id is an integer
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv')
print('Train file: ', train_file)
valid_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')
print('Valid file: ', valid_file)
test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')
print('test file: ', test_file)

"""Let's see train, valid, test file"""

train_df = pd.read_csv(train_file)
print(train_df.head(5))

print(train_df.info())
print(train_df["label"].unique())
print(train_df["label"].value_counts())

print(train_df.tweet[0])

valid_df = pd.read_csv(valid_file)
valid_df.head(5)

print(valid_df.info())
print(valid_df["label"].unique())
print(valid_df["label"].value_counts())

test_df = pd.read_csv(test_file)
test_df.head(5)

print(test_df.info())
print(test_df["label"].unique())
print(test_df["label"].value_counts())

"""#Data pre processing"""

import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text):
    """
    A function that preprocesses text data by removing stopwords, punctuation, and performing lemmatization.

    Args:
    text: The text data to be preprocessed.

    Returns:
    The preprocessed text data.
    """
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Convert text to lowercase
    text = text.lower()

    # Tokenize the text
    tokens = nltk.word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if not word in stop_words]

    # Lemmatize the text
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Join the tokens back into a string
    text = ' '.join(tokens)

    return text

# Apply the preprocess_text function to the 'tweet' column
#train_df['tweet'] = train_df['tweet'].apply(preprocess_text)

# Print the preprocessed text
#print(train_df.head(5))

"""from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
# Creating a TfidfVectorizer object
vectorizer = TfidfVectorizer()

# Fitting and transforming the vectorizer on the preprocessed text data
X_train = vectorizer.fit_transform(train_df['tweet'])

# Encoding the label data
le = LabelEncoder()
y_train = le.fit_transform(train_df['label'])


# Preprocessing the text data in the validation data, loaded earlier
valid_df['tweet'] = valid_df['tweet'].apply(preprocess_text)

# Transforming the preprocessed text data using the fitted vectorizer
X_valid = vectorizer.transform(valid_df['tweet'])

# Encoding the label data in the validation data
y_valid = le.transform(valid_df['label'])

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Create a logistic regression object
lr = LogisticRegression()

# Fit the logistic regression model on the training data
lr.fit(X_train, y_train)

# Predict the labels of the validation data using the trained model
y_pred = lr.predict(X_valid)

# Evaluate the model's performance on the validation data
accuracy = accuracy_score(y_valid, y_pred)
print("Accuracy:", accuracy)

# Generate a confusion matrix
cm = confusion_matrix(y_valid, y_pred)
print("confusion matrix \n", cm)

# Generate a classification report
cr = classification_report(y_valid, y_pred)
print(cr)

# Plot the confusion matrix as a heatmap
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d", xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()

improve the performance of the logistic regression model by tuning its hyperparameters. The two main hyperparameters of logistic regression are the regularization parameter C and the penalty function penalty.

from sklearn.model_selection import GridSearchCV

# Create a logistic regression object
lr = LogisticRegression()

# Define a grid of hyperparameters to search over
param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}

# Create a grid search object
grid_search = GridSearchCV(lr, param_grid, cv=5)

# Fit the grid search object on the training data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and the corresponding accuracy score
print("Best Hyperparameters:", grid_search.best_params_)
print("Best Accuracy Score:", grid_search.best_score_)

# Predict the labels of the validation data using the best model
y_pred = grid_search.best_estimator_.predict(X_valid)

# Evaluate the model's performance on the validation data
accuracy = accuracy_score(y_valid, y_pred)
print("Accuracy:", accuracy)

# Generate a confusion matrix
cm = confusion_matrix(y_valid, y_pred)

# Generate a classification report
cr = classification_report(y_valid, y_pred)
print(cr)

# Plot the confusion matrix as a heatmap
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d", xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()
"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

def compute_performance(y_true, y_pred):

    # Calculate the F1-score
    f1 = f1_score(y_true, y_pred, average='macro')

    # Calculate other performance metrics
    accuracy = accuracy_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred, average='macro')
    precision = precision_score(y_true, y_pred, average='macro')

    # Print the performance metrics
    print("Accuracy:", accuracy)
    print("Recall:", recall)
    print("Precision:", precision)
    print("F1 Score:", f1)

    # Generate a confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Plot the confusion matrix as a heatmap
    sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.title("Confusion Matrix of Random forest Classifier")
    plt.show()

    return f1

"""#train method 1"""

import pickle
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
def train_method1(train_file, val_file, model_dir):

    # Preprocess the text data
    train_df['tweet'] = train_df['tweet'].apply(preprocess_text)

    # Create a TfidfVectorizer object
    vectorizer = TfidfVectorizer()
    
    # Fit and transform the vectorizer on the preprocessed text data
    X_train = vectorizer.fit_transform(train_df['tweet'])

    # Encode the label data
    le = LabelEncoder()
    y_train = le.fit_transform(train_df['label'])

    # Create a logistic regression object
    lr = LogisticRegression()
    rf = RandomForestClassifier()

    # Train the logistic regression model on the training data
    #lr.fit(X_train, y_train)
    rf.fit(X_train, y_train)
    # Load the validation data
    valid_df = pd.read_csv(val_file)

    # Preprocess the text data in the validation data
    valid_df['tweet'] = valid_df['tweet'].apply(preprocess_text)

    # Transform the preprocessed text data using the fitted vectorizer
    X_valid = vectorizer.transform(valid_df['tweet'])

    # Encode the label data in the validation data
    y_valid = le.transform(valid_df['label'])

    # Predict the labels of the validation data using the trained model
    y_pred = rf.predict(X_valid)

    # Compute the performance of the model using the compute_performance function
    f1 = compute_performance(y_valid, y_pred)

    # Save the fitted vectorizer and trained logistic regression model in the model directory
    with open(os.path.join(model_dir, 'vectorizer.pkl'), 'wb') as f:
        pickle.dump(vectorizer, f)

    with open(os.path.join(model_dir, 'model.pkl'), 'wb') as f:
        pickle.dump(rf, f)

    return f1

MODEL_1_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '1') # Model 1 directory
print('Model 1 directory: ', MODEL_1_DIRECTORY)

"""### training with original dataset"""

train_method1(train_file, valid_file, MODEL_1_DIRECTORY)

"""#### Trying to improve the model with GridSearchCV"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

def train_method1_grid_search(train_file, val_file, model_dir):

    # Preprocess the text data
    train_df['tweet'] = train_df['tweet'].apply(preprocess_text)

    # Create a TfidfVectorizer object
    vectorizer = TfidfVectorizer()

    # Fit and transform the vectorizer on the preprocessed text data
    X_train = vectorizer.fit_transform(train_df['tweet'])

    # Encode the label data
    le = LabelEncoder()
    y_train = le.fit_transform(train_df['label'])

    # Define a parameter grid for hyperparameter tuning
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 4],
        'min_samples_leaf': [1, 2]
    }

    # Create a random forest classifier object
    rf = RandomForestClassifier()

    # Perform hyperparameter tuning using GridSearchCV
    grid_search = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1, scoring='f1_macro')
    grid_search.fit(X_train, y_train)

    # Print the best hyperparameters found during hyperparameter tuning
    print('Best hyperparameters:', grid_search.best_params_)

    # Train the random forest classifier on the training data using the best hyperparameters
    rf_best = RandomForestClassifier(**grid_search.best_params_)
    rf_best.fit(X_train, y_train)

    # Load the validation data
    valid_df = pd.read_csv(val_file)

    # Preprocess the text data in the validation data
    valid_df['tweet'] = valid_df['tweet'].apply(preprocess_text)

    # Transform the preprocessed text data using the fitted vectorizer
    X_valid = vectorizer.transform(valid_df['tweet'])

    # Encode the label data in the validation data
    y_valid = le.transform(valid_df['label'])

    # Predict the labels of the validation data using the trained model
    y_pred = rf_best.predict(X_valid)

    # Compute the performance of the model using the compute_performance function
    f1 = compute_performance(y_valid, y_pred)

    # Save the fitted vectorizer and trained random forest classifier model in the model directory
    with open(os.path.join(model_dir, 'vectorizer_gs.pkl'), 'wb') as f:
        pickle.dump(vectorizer, f)

    with open(os.path.join(model_dir, 'model_gs.pkl'), 'wb') as f:
        pickle.dump(rf_best, f)

    return f1

train_method1_grid_search(train_file, valid_file, MODEL_1_DIRECTORY)

"""### Spliting original dataset"""

MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25') # Model 1 trained using 25% of train data directory
print('Model 1 directory with 25% data: ', MODEL_1_25_DIRECTORY)

from sklearn.model_selection import train_test_split

# Loading the original dataset
train_file_02 = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv')
train_df_02 = pd.read_csv(train_file)

# Get the labels and features
X = train_df_02['tweet']
y = train_df_02['label']

# Split the data into train and test sets with a 75:25 split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=student_id)

# Split the train data into 4 subsets
X_train_temp, X_train_100, y_train_temp, y_train_100 = train_test_split(X_train, y_train, test_size=0.75, stratify=y_train, random_state=student_id)
X_train_25, X_train_temp, y_train_25, y_train_temp = train_test_split(X_train_temp, y_train_temp, test_size=0.67, stratify=y_train_temp, random_state=student_id)
X_train_50, X_train_75, y_train_50, y_train_75 = train_test_split(X_train_temp, y_train_temp, test_size=0.5, stratify=y_train_temp, random_state=student_id)

# Save the train subsets in Google Drive
MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25') # Model 1 trained using 25% of train data directory
train_25_file = os.path.join(MODEL_1_25_DIRECTORY, 'train_25.csv')
pd.DataFrame({'tweet': X_train_25, 'label': y_train_25}).to_csv(train_25_file, index=False)

MODEL_1_50_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'50') # Model 1 trained using 50% of train data directory
train_50_file = os.path.join(MODEL_1_50_DIRECTORY, 'train_50.csv')
pd.DataFrame({'tweet': X_train_50, 'label': y_train_50}).to_csv(train_50_file, index=False)

MODEL_1_75_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'75') # Model 1 trained using 75% of train data directory
train_75_file = os.path.join(MODEL_1_75_DIRECTORY, 'train_75.csv')
pd.DataFrame({'tweet': X_train_75, 'label': y_train_75}).to_csv(train_75_file, index=False)

MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'100') # Model 1 trained using 25% of train data directory
train_100_file = os.path.join(MODEL_1_100_DIRECTORY, 'train_100.csv')
pd.DataFrame({'tweet': X_train_100, 'label': y_train_100}).to_csv(train_100_file, index=False)

def show_dataset_info(file_path):
    # Read the CSV file into a pandas DataFrame
    df = pd.read_csv(file_path)

    # Display information about the dataset
    print("Dataset Info:")
    print(df.info())

    # Display the unique values of the "label" column's data
    print("\nUnique Values of 'label' Column:")
    print(df['label'].value_counts())
    print(df['label'].unique())

df = pd.read_csv(train_75_file)
df["label"].value_counts()

show_dataset_info(train_25_file)

show_dataset_info(train_50_file)

show_dataset_info(train_75_file)

show_dataset_info(train_100_file)

"""### training with 25% data"""

MODEL_1_25_saving_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/25"
print('Model 1 25% data directory: ', MODEL_1_25_saving_DIRECTORY)
train_method1(train_25_file, valid_file, MODEL_1_25_saving_DIRECTORY)

"""### training with 50% data"""

MODEL_1_50_saving_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/50"
print('Model 1 50% data directory: ', MODEL_1_50_saving_DIRECTORY)
train_method1(train_50_file, valid_file, MODEL_1_50_saving_DIRECTORY)

"""### training with 75% data"""

MODEL_1_75_saving_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/75"
print('Model 1 75% data directory: ', MODEL_1_75_saving_DIRECTORY)
train_method1(train_75_file, valid_file, MODEL_1_75_saving_DIRECTORY)

"""### training with 100% data"""

MODEL_1_100_saving_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/100"
print('Model 1 100% data directory: ', MODEL_1_100_saving_DIRECTORY)
train_method1(train_100_file, valid_file, MODEL_1_100_saving_DIRECTORY)

"""## Testing Method 1 Code
Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  
"""

model_1_25_output_test_file = os.path.join(MODEL_1_25_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data 
print('Output file name using model 1 using 25% of train data: ',model_1_25_output_test_file)

test_file

import os
import pandas as pd
import pickle
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
"""
def test_method1(test_file, model_file, output_dir):
    # Load the test data
    test_df = pd.read_csv(test_file)

    # Load the trained model
    with open(model_file, 'rb') as f:
        model = pickle.load(f)

    # Load the fitted vectorizer
    with open(os.path.join(os.path.dirname(model_file), 'vectorizer.pkl'), 'rb') as f:
        vectorizer = pickle.load(f)

    # Preprocess the text data in the test data
    test_df['tweet'] = test_df['tweet'].apply(preprocess_text)

    # Transform the preprocessed text data using the fitted vectorizer
    X_test = vectorizer.transform(test_df['tweet'])

    # Encode the label data in the test data
    le = LabelEncoder()
    y_test = le.fit_transform(test_df['label'])

    # Predict the labels of the test data using the loaded model
    y_pred = model.predict(X_test)

    # Compute evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')

    # Print the evaluation metrics
    print('Accuracy: {:.4f}'.format(accuracy))
    print('Precision: {:.4f}'.format(precision))
    print('Recall: {:.4f}'.format(recall))
    print('F1 Score: {:.4f}'.format(f1))

    # Plot the confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()


    # Save the output in a CSV format in the output directory
    output_file = os.path.join(output_dir, 'output_test.csv')
    output_df = pd.DataFrame({'tweet': test_df['tweet'], 'label': test_df['label'], 'predicted_label': y_pred})
    output_df.to_csv(output_file, index=False)

    return

    """

import os
import pandas as pd
import pickle
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def test_method1(test_file, model_file, output_dir):
    # Load the test data
    test_df = pd.read_csv(test_file)

    # Load the trained model
    with open(model_file, 'rb') as f:
        model = pickle.load(f)

    # Load the fitted vectorizer
    with open(os.path.join(os.path.dirname(model_file), 'vectorizer.pkl'), 'rb') as f:
        vectorizer = pickle.load(f)

    # Preprocess the text data in the test data
    test_df['tweet'] = test_df['tweet'].apply(preprocess_text)

    # Transform the preprocessed text data using the fitted vectorizer
    X_test = vectorizer.transform(test_df['tweet'])

    # Encode the label data in the test data
    le = LabelEncoder()
    y_test = le.fit_transform(test_df['label'])

    # Predict the labels of the test data using the loaded model
    y_pred = model.predict(X_test)

    # Compute evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')

    # Print the evaluation metrics
    print('Accuracy: {:.4f}'.format(accuracy))
    print('Precision: {:.4f}'.format(precision))
    print('Recall: {:.4f}'.format(recall))
    print('F1 Score: {:.4f}'.format(f1))

    # Plot the confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Save the output in a CSV format in the output directory
    output_file = os.path.join(output_dir, 'output_test.csv')
    output_df = pd.DataFrame({'tweet': test_df['tweet'], 'label': test_df['label'], 'predicted_label': y_pred})
    output_df.to_csv(output_file, index=False)

    return accuracy, precision, recall, f1

def plot_performance(train_results, test_results, metric):
    # Set the figure size
    plt.figure(figsize=(8, 6))

    # Plot the training performance
    plt.plot(train_results['train_size'], train_results[metric], label='Training Set', marker='o')

    # Plot the testing performance
    plt.plot(test_results['train_size'], test_results[metric], label='Testing Set', marker='o')

    # Set the plot title, X and Y labels
    plt.title('Performance on Training and Testing Sets')
    plt.xlabel('Data Size')
    plt.ylabel(metric)

    # Set the X ticks
    xticks = np.arange(0, 1.1, 0.1)
    plt.xticks(xticks, ['{}%'.format(int(x*100)) for x in xticks])

    # Add annotations to the plot
    for i, (train_size, train_value) in enumerate(zip(train_results['train_size'], train_results[metric])):
        test_value = test_results[metric][i]
        plt.annotate(xy=(train_size, train_value), s='{:.4f}'.format(train_value), ha='right', va='bottom')
        plt.annotate(xy=(train_size, test_value), s='{:.4f}'.format(test_value), ha='left', va='top')

    # Use a curve graph instead of a line graph
    plt.plot(train_results['train_size'], train_results[metric], 'bo-', alpha=0.5)
    plt.plot(test_results['train_size'], test_results[metric], 'ro-', alpha=0.5)

    # Set the legend
    plt.legend()

    # Show the plot
    plt.show()
    return

MODEL_1_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/model.pkl"
output_dir = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1"
test_method1(test_file, MODEL_1_DIRECTORY, output_dir)

"""testing with 25% training dataset model

"""

MODEL_1_25_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/25/model.pkl"
output_dir_25 = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/25"
test_method1(test_file, MODEL_1_25_DIRECTORY, output_dir_25)

"""testing with 50% training model"""

MODEL_1_50_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/50/model.pkl"
output_dir_50 = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/50"
test_method1(test_file, MODEL_1_50_DIRECTORY, output_dir_50)

"""testing with 75% training model"""

MODEL_1_75_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/75/model.pkl"
output_dir_75 = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/75"
test_method1(test_file, MODEL_1_75_DIRECTORY, output_dir_75)

"""testing with 100% training model"""

MODEL_1_100_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/100/model.pkl"
output_dir_100 = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/100"
test_method1(test_file, MODEL_1_100_DIRECTORY, output_dir_100)

"""## Method 1 End

#Method 2 Start
"""

from sklearn.ensemble import GradientBoostingClassifier

def train_method2(train_file, val_file, model_dir):
    # Load the train data
    train_df = pd.read_csv(train_file)

    # Preprocess the text data
    train_df['tweet'] = train_df['tweet'].apply(preprocess_text)

    # Create a TfidfVectorizer object
    vectorizer = TfidfVectorizer()

    # Fit and transform the vectorizer on the preprocessed text data
    X_train = vectorizer.fit_transform(train_df['tweet'])

    # Encode the label data
    le = LabelEncoder()
    y_train = le.fit_transform(train_df['label'])

    # Create a Gradient Boosting classifier object
    gbc = GradientBoostingClassifier()

    # Train the Gradient Boosting classifier model on the training data
    gbc.fit(X_train, y_train)

    # Load the validation data
    valid_df = pd.read_csv(val_file)

    # Preprocess the text data in the validation data
    valid_df['tweet'] = valid_df['tweet'].apply(preprocess_text)

    # Transform the preprocessed text data using the fitted vectorizer
    X_valid = vectorizer.transform(valid_df['tweet'])

    # Encode the label data in the validation data
    y_valid = le.transform(valid_df['label'])

    # Predict the labels of the validation data using the trained model
    y_pred = gbc.predict(X_valid)

    # Compute the performance of the model using the compute_performance function
    f1 = compute_performance(y_valid, y_pred)

    # Save the fitted vectorizer and trained Gradient Boosting classifier model in the model directory
    with open(os.path.join(model_dir, 'vectorizer.pkl'), 'wb') as f:
        pickle.dump(vectorizer, f)

    with open(os.path.join(model_dir, 'model.pkl'), 'wb') as f:
        pickle.dump(gbc, f)

    return f1

MODEL_2_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '2') # Model 1 directory
print('Model 2 directory: ', MODEL_2_DIRECTORY)

train_method2(train_file, valid_file, MODEL_2_DIRECTORY)

"""###Trying to improve the results of Method 2"""

import os
import pandas as pd
import pickle
from sklearn.metrics import f1_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV

def train_method2_gsc(train_file, val_file, model_dir):
    # Load the train data
    train_df = pd.read_csv(train_file)

    # Preprocess the text data
    train_df['tweet'] = train_df['tweet'].apply(preprocess_text)

    # Create a TfidfVectorizer object
    vectorizer = TfidfVectorizer()

    # Fit and transform the vectorizer on the preprocessed text data
    X_train = vectorizer.fit_transform(train_df['tweet'])

    # Encode the label data
    le = LabelEncoder()
    y_train = le.fit_transform(train_df['label'])

    # Create a XGBoost classifier object
    xgb = XGBClassifier(use_label_encoder=False)

    # Set the hyperparameters to tune
    param_grid = {
        'max_depth': [3, 4, 5, 6],
        'n_estimators': [50, 100, 150, 200],
        'learning_rate': [0.01, 0.1, 0.5]
    }

    # Create a GridSearchCV object to find the best hyperparameters
    grid_search = GridSearchCV(xgb, param_grid, cv=5, scoring='f1_macro')
    grid_search.fit(X_train, y_train)

    # Print the best hyperparameters found
    print('Best hyperparameters:', grid_search.best_params_)

    # Train the XGBoost classifier model on the training data using the best hyperparameters
    xgb = XGBClassifier(**grid_search.best_params_, use_label_encoder=False)
    xgb.fit(X_train, y_train)

    # Load the validation data
    valid_df = pd.read_csv(val_file)

    # Preprocess the text data in the validation data
    valid_df['tweet'] = valid_df['tweet'].apply(preprocess_text)

    # Transform the preprocessed text data using the fitted vectorizer
    X_valid = vectorizer.transform(valid_df['tweet'])

    # Encode the label data in the validation data
    y_valid = le.transform(valid_df['label'])

    # Predict the labels of the validation data using the trained model
    y_pred = xgb.predict(X_valid)

    # Compute the performance of the model using the compute_performance function
    f1 = compute_performance(y_valid, y_pred)

    # Save the fitted vectorizer and trained XGBoost classifier model in the model directory
    with open(os.path.join(model_dir, 'vectorizer.pkl'), 'wb') as f:
        pickle.dump(vectorizer, f)

    with open(os.path.join(model_dir, 'model.pkl'), 'wb') as f:
        pickle.dump(xgb, f)

    return f1

train_method2_gsc(train_file, valid_file, MODEL_2_DIRECTORY)

"""### training 4 splitted sub sets in Method 2"""

train_25_file

MODEL_2_25_saving_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2/25"
print('Model 2 25% data directory: ', MODEL_2_25_saving_DIRECTORY)
train_method2(train_25_file, valid_file, MODEL_2_25_saving_DIRECTORY)

MODEL_2_50_saving_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2/50"
print('Model 2 50% data directory: ', MODEL_2_50_saving_DIRECTORY)
train_method2(train_50_file, valid_file, MODEL_2_50_saving_DIRECTORY)

MODEL_2_75_saving_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2/75"
print('Model 2 75% data directory: ', MODEL_2_75_saving_DIRECTORY)
train_method2(train_75_file, valid_file, MODEL_2_75_saving_DIRECTORY)

MODEL_2_100_saving_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2/100"
print('Model 2 100% data directory: ', MODEL_2_100_saving_DIRECTORY)
train_method2(train_100_file, valid_file, MODEL_2_100_saving_DIRECTORY)

"""### testing with method 2 model"""

def test_method2(test_file, model_file, output_dir):
  # Load the test data
  test_df = pd.read_csv(test_file)

  # Load the trained model
  with open(model_file, 'rb') as f:
    model = pickle.load(f)

# Load the fitted vectorizer
  with open(os.path.join(os.path.dirname(model_file), 'vectorizer.pkl'), 'rb') as f:
    vectorizer = pickle.load(f)

# Preprocess the text data in the test data
  test_df['tweet'] = test_df['tweet'].apply(preprocess_text)

# Transform the preprocessed text data using the fitted vectorizer
  X_test = vectorizer.transform(test_df['tweet'])

# Encode the label data in the test data
  le = LabelEncoder()
  y_test = le.fit_transform(test_df['label'])

# Predict the labels of the test data using the loaded model
  y_pred = model.predict(X_test)

# Compute evaluation metrics
  accuracy = accuracy_score(y_test, y_pred)
  precision = precision_score(y_test, y_pred, average='macro')
  recall = recall_score(y_test, y_pred, average='macro')
  f1 = f1_score(y_test, y_pred, average='macro')

# Print the evaluation metrics
  print('Accuracy: {:.4f}'.format(accuracy))
  print('Precision: {:.4f}'.format(precision))
  print('Recall: {:.4f}'.format(recall))
  print('F1 Score: {:.4f}'.format(f1))

# Plot the confusion matrix
  cm = confusion_matrix(y_test, y_pred)
  sns.heatmap(cm, annot=True, fmt='d')
  plt.xlabel('Predicted')
  plt.ylabel('True')
  plt.title('Confusion Matrix')
  plt.show()

# Plot the ROC curve
  y_score = model.predict_proba(X_test)[:, 1]
  fpr, tpr, _ = roc_curve(y_test, y_score)
  roc_auc = auc(fpr, tpr)
  plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = {:.2f})'.format(roc_auc))
  plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('Receiver Operating Characteristic Curve')
  plt.legend(loc="lower right")
  plt.show()

# Save the output in a CSV format in the output directory
  output_file = os.path.join(output_dir, 'output_test.csv')
  output_df = pd.DataFrame({'tweet': test_df['tweet'], 'label': test_df['label'], 'predicted_label': y_pred})
  output_df.to_csv(output_file, index=False)

  return

MODEL_2_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/1/model.pkl"
output_dir = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2"
test_method2(test_file, MODEL_2_DIRECTORY, output_dir)

"""### Testing_method2 with 4 splitted sub sets of original data"""

MODEL_2_25_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2/25/model.pkl"
output_dir_25 = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2/25"
test_method2(test_file, MODEL_2_25_DIRECTORY, output_dir_25)

MODEL_2_50_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2/50/model.pkl"
output_dir_50 = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2/50"
test_method2(test_file, MODEL_2_50_DIRECTORY, output_dir_50)

MODEL_2_75_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2/75/model.pkl"
output_dir_75 = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2/75"
test_method2(test_file, MODEL_2_75_DIRECTORY, output_dir_75)

MODEL_2_100_DIRECTORY = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2/100/model.pkl"
output_dir_100 = "/content/gdrive/MyDrive/CE807_7_SP/assignment2/2201277/models/2/100"
test_method2(test_file, MODEL_2_100_DIRECTORY, output_dir_100)

"""### Method 2 End"""